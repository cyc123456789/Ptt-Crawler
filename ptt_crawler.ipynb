{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from six import u\n",
    "\n",
    "PTT_URL = 'https://www.ptt.cc'\n",
    "\n",
    "\n",
    "def get_web_page(url):\n",
    "    resp = requests.get(url=url,\n",
    "                        cookies={'over18': '1'})\n",
    "    if resp.status_code != 200:\n",
    "        print('Invalid url:', resp.url)\n",
    "        return None\n",
    "    else:\n",
    "        return resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_articles(dom):\n",
    "    soup = BeautifulSoup(dom, 'html.parser')\n",
    "    #print(dom)\n",
    "    # 上頁\n",
    "    paging_div = soup.find('div', 'btn-group btn-group-paging')\n",
    "    prev_url = paging_div.find_all('a')[1]['href']\n",
    "    #print('Prev url:',prev_url)\n",
    "\n",
    "    articles = []\n",
    "    divs = soup.find_all('div', 'r-ent')\n",
    "    #print([d.text for d in divs ])\n",
    "    for d in divs:\n",
    "\n",
    "        #if '[食記]' in d.find('div',class_='title').text.strip():\n",
    "        #if d.find('div', class_='date').text.strip() == date:\n",
    "\n",
    "        push_count = 0\n",
    "        push_str = d.find('div', 'nrec').text\n",
    "        if push_str:\n",
    "            try:\n",
    "                push_count = int(push_str)\n",
    "            except ValueError:\n",
    "\n",
    "                if push_str == '爆':\n",
    "                    push_count = 99\n",
    "                elif push_str.startswith('X'):\n",
    "                    push_count = -10\n",
    "\n",
    "        if d.find('a'):\n",
    "            href = d.find('a')['href']\n",
    "            title = d.find('a').text\n",
    "            date = d.find('div', class_='date').text.strip()\n",
    "            author = d.find('div', 'author').text if d.find('div', 'author') else ''\n",
    "            articles.append({\n",
    "                'date': date,\n",
    "                'title': title,\n",
    "                'href': href,\n",
    "                'push_count': push_count,\n",
    "                'author': author\n",
    "            })\n",
    "\n",
    "    return articles, prev_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_author_ids(posts, pattern):\n",
    "    ids = set()\n",
    "    for post in posts:\n",
    "        if pattern in post['author']:\n",
    "            ids.add(post['author'])\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_articles_detail(link):\n",
    "    single_page = get_web_page(link)\n",
    "\n",
    "    soup = BeautifulSoup(single_page, 'html.parser')\n",
    "    main_content = soup.find(id=\"main-content\")\n",
    "    metas = main_content.select('div.article-metaline')\n",
    "    author = ''\n",
    "    title = ''\n",
    "    date = ''\n",
    "    article_id = re.sub('\\.html', '', link.split('/')[-1])\n",
    "    board = main_content.select('div.article-metaline-right')[0].find(class_='article-meta-value').text\n",
    "    if metas:\n",
    "        author = metas[0].select('span.article-meta-value')[0].string if metas[0].select('span.article-meta-value')[\n",
    "            0] else author\n",
    "        title = metas[1].select('span.article-meta-value')[0].string if metas[1].select('span.article-meta-value')[\n",
    "            0] else title\n",
    "        date = metas[2].select('span.article-meta-value')[0].string if metas[2].select('span.article-meta-value')[\n",
    "            0] else date\n",
    "\n",
    "        # remove meta nodes\n",
    "        for meta in metas:\n",
    "            meta.extract()\n",
    "        for meta in main_content.select('div.article-metaline-right'):\n",
    "            meta.extract()\n",
    "\n",
    "    # remove and keep push nodes\n",
    "    pushes = main_content.find_all('div', class_='push')\n",
    "    for push in pushes:\n",
    "        push.extract()\n",
    "\n",
    "    try:\n",
    "        ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "        ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "    except:\n",
    "        ip = \"None\"\n",
    "\n",
    "    # 移除 '※ 發信站:' (starts with u'\\u203b'), '◆ From:' (starts with u'\\u25c6'), 空行及多餘空白\n",
    "    # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "    filtered = [v for v in main_content.stripped_strings if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']]\n",
    "    expr = re.compile(\n",
    "        u(r'[^\\u4e00-\\u9fa5\\u3002\\uff1b\\uff0c\\uff1a\\u201c\\u201d\\uff08\\uff09\\u3001\\uff1f\\u300a\\u300b\\s\\w:/-_.?~%()]'))\n",
    "    for i in range(len(filtered)):\n",
    "        filtered[i] = re.sub(expr, '', filtered[i])\n",
    "\n",
    "    filtered = [_f for _f in filtered if _f]  # remove empty strings\n",
    "    #filtered = [x for x in filtered if article_id not in x]  # remove last line containing the url of the article\n",
    "    content = ' '.join(filtered)\n",
    "    content = re.sub(r'(\\s)+', ' ', content)\n",
    "    # print 'content', content\n",
    "\n",
    "    # push messages\n",
    "    p, b, n = 0, 0, 0\n",
    "    messages = []\n",
    "    for push in pushes:\n",
    "        if not push.find('span', 'push-tag'):\n",
    "            continue\n",
    "        push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "        push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "        # if find is None: find().strings -> list -> ' '.join; else the current way\n",
    "        push_content = push.find('span', 'push-content').strings\n",
    "        push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')  # remove ':'\n",
    "        push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "        messages.append({'push_tag': push_tag, 'push_userid': push_userid, 'push_content': push_content,\n",
    "                         'push_ipdatetime': push_ipdatetime})\n",
    "        if push_tag == u'推':\n",
    "            p += 1\n",
    "        elif push_tag == u'噓':\n",
    "            b += 1\n",
    "        else:\n",
    "            n += 1\n",
    "\n",
    "    # count: 推噓文相抵後的數量; all: 推文總數\n",
    "    message_count = {'all': p + b + n, 'count': p - b, 'push': p, 'boo': b, \"neutral\": n}\n",
    "\n",
    "    # print 'msgs', messages\n",
    "    # print 'mscounts', message_count\n",
    "\n",
    "    # json data\n",
    "    data = []\n",
    "    data.append({\n",
    "        'url': link,\n",
    "        'board': board,\n",
    "        'article_id': article_id,\n",
    "        'article_title': title,\n",
    "        'author': author,\n",
    "        'date': date,\n",
    "        'content': content,\n",
    "        'ip': ip,\n",
    "        'message_count': message_count,\n",
    "        'messages': messages\n",
    "    })\n",
    "    # print 'original:', d\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有 8 篇文章\n",
      "{'date': '3/28', 'title': '[食記] 台北 晶華酒店 栢麗廳 晚餐', 'href': '/bbs/Food/M.1648458856.A.476.html', 'push_count': 0, 'author': 'osakaleo'}\n",
      "{'date': '3/28', 'title': 'Fw: [食記] 新竹 好滋味鹽酥雞串燒 ～街邊日式居酒屋', 'href': '/bbs/Food/M.1648462045.A.0D6.html', 'push_count': 0, 'author': 'pure816'}\n",
      "{'date': '3/28', 'title': '[食記] 基隆市信義區-森之秋精緻鍋物', 'href': '/bbs/Food/M.1648470224.A.097.html', 'push_count': 0, 'author': 'fatlee'}\n",
      "{'date': '3/28', 'title': '[食記] 彰化溪湖-楊仔頭羊肉爐 ', 'href': '/bbs/Food/M.1648470767.A.9D5.html', 'push_count': 0, 'author': 'jasmine1112'}\n",
      "{'date': '12/16', 'title': '[公告] Food板 板規 V3.91', 'href': '/bbs/Food/M.1355673582.A.5F7.html', 'push_count': 0, 'author': 'Dilbert'}\n",
      "{'date': '9/28', 'title': '[公告] 發文請在標題加上地區及提供地址電話。^^', 'href': '/bbs/Food/M.1190944426.A.E6C.html', 'push_count': 18, 'author': 'bluefish'}\n",
      "{'date': '10/01', 'title': '[公告] 文章被刪除者請洽精華區的資源回收桶', 'href': '/bbs/Food/M.1128132666.A.0FD.html', 'push_count': 0, 'author': 'Dilbert'}\n",
      "{'date': '6/04', 'title': '[公告] 新增板規22：發文禁附延伸閱讀連結', 'href': '/bbs/Food/M.1496532469.A.C36.html', 'push_count': 4, 'author': 'Dilbert'}\n",
      "https://www.ptt.cc/bbs/Food/M.1648458856.A.476.html\n",
      "https://www.ptt.cc/bbs/Food/M.1648462045.A.0D6.html\n",
      "https://www.ptt.cc/bbs/Food/M.1648470224.A.097.html\n",
      "https://www.ptt.cc/bbs/Food/M.1648470767.A.9D5.html\n",
      "https://www.ptt.cc/bbs/Food/M.1355673582.A.5F7.html\n",
      "https://www.ptt.cc/bbs/Food/M.1190944426.A.E6C.html\n",
      "https://www.ptt.cc/bbs/Food/M.1128132666.A.0FD.html\n",
      "https://www.ptt.cc/bbs/Food/M.1496532469.A.C36.html\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    current_page = get_web_page(PTT_URL + '/bbs/Food/index.html')\n",
    "    articles = []\n",
    "    detail_articles = []\n",
    "    if current_page:\n",
    "        page_count = 0\n",
    "        # 抓幾頁\n",
    "        page_max = 1\n",
    "        current_articles, prev_url = get_articles(current_page)\n",
    "\n",
    "        while current_articles and (page_count < page_max):\n",
    "            #print('Round:', page_count)\n",
    "            articles += current_articles\n",
    "            current_page = get_web_page(PTT_URL + prev_url)\n",
    "            current_articles, prev_url = get_articles(current_page)\n",
    "            page_count = page_count + 1\n",
    "            #print('Next Round:', page_count)\n",
    "\n",
    "        # print(get_author_ids(articles, '5566'))\n",
    "        print('有', len(articles), '篇文章')\n",
    "        for a in articles:\n",
    "            #if int(a['push_count']) > threshold:\n",
    "            print(a)\n",
    "        with open('title.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(articles, f, indent=2, sort_keys=True, ensure_ascii=False)\n",
    "\n",
    "        # TODO: 抓文章內容\n",
    "    if articles:\n",
    "        for a in articles:\n",
    "            link = PTT_URL + a['href']\n",
    "            print(link)\n",
    "            current_detail_articles = get_articles_detail(link)\n",
    "            detail_articles += current_detail_articles\n",
    "        with open('detail_articles.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(detail_articles, f, indent=2, sort_keys=True, ensure_ascii=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}